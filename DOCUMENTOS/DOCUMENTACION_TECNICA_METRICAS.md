# üìä DOCUMENTACI√ìN T√âCNICA - METRICAS DE CALIDAD DE DATOS

## Tabla de Contenidos
1. [Resumen General](#resumen-general)
2. [Metricas Implementadas](#metricas-implementadas)
3. [Flujo de Operaci√≥n](#flujo-de-operaci√≥n)
4. [Detalles T√©cnicos por Metrica](#detalles-t√©cnicos-por-m√©trica)

---

## üéØ Resumen General

Este proyecto es un **API de Evaluaci√≥n de Calidad de Datos** que calcula **14 m√©tricas de calidad** para datasets publicados en la plataforma **datos.gov.co**. Cada m√©trica eval√∫a un aspecto diferente de la calidad de datos siguiendo las directrices de la **Gu√≠a MinTIC 2025**.

### Caracter√≠sticas Principales:
- ‚úÖ **C√°lculos basados en metadata**: La mayor√≠a de m√©tricas no requieren cargar datos completos
- ‚úÖ **Opci√≥n de carga completa**: Algunos endpoints requieren `/load_data` para an√°lisis profundos
- ‚úÖ **Evaluaci√≥n 0-10**: Todas las m√©tricas devuelven scores normalizados
- ‚úÖ **API REST**: Endpoints simples y RESTful
- ‚úÖ **Paginaci√≥n optimizada**: Manejo eficiente de grandes datasets

---

## üìã Metricas Implementadas

| # | M√©trica | Escala | Requiere Datos | Descripci√≥n |
|---|---------|--------|-----------------|-------------|
| 1 | **Actualidad** | 0-10 | No | ¬øQu√© tan reciente es la informaci√≥n? |
| 2 | **Accesibilidad** | 0-10 | No | ¬øQu√© tan f√°cil es acceder al dataset? |
| 3 | **Confidencialidad** | 0-10 | No | ¬øEst√°n protegidos los datos sensibles? |
| 4 | **Completitud** | 0-10 | S√≠* | ¬øCu√°ntos valores est√°n presente? |
| 5 | **Conformidad** | 0-1 | S√≠* | ¬øCumple est√°ndares y convenciones? |
| 6 | **Trazabilidad** | 0-10 | No | ¬øSe puede auditar la informaci√≥n? |
| 7 | **Disponibilidad** | 0-10 | No | ¬øEst√° siempre listo y accesible? |
| 8 | **Portabilidad** | 0-10 | S√≠* | ¬øSe puede mover entre sistemas? |
| 9 | **Credibilidad** | 0-10 | S√≠* | ¬øEs confiable la fuente? |
| 10 | **Recuperabilidad** | 0-10 | S√≠* | ¬øSe puede recuperar f√°cilmente? |
| 11 | **Unicidad** | 0-10 | S√≠* | ¬øHay duplicados o datos repetidos? |
| 12 | **Relevancia** | 0-10 | No | ¬øProporciona valor decisional? |
| 13 | **Precisi√≥n** | 0-10 | S√≠* | ¬øNivel apropiado de desagregaci√≥n? |
| 14 | **Consistencia** | 0-10 | S√≠* | ¬øHay coherencia en los datos? |

*Nota: "S√≠*" significa que opcionalmente requiere datos si est√° disponible, pero intenta funcionar con solo metadata

---

## üîÑ Flujo de Operaci√≥n

### Paso 1: Inicializaci√≥n
```
POST /initialize
{
  "dataset_id": "ijus-ubej",
  "load_full": false  // Por defecto no carga datos completos
}
```

**Qu√© sucede:**
- Obtiene metadatos desde Socrata API
- Crea instancia de `DataQualityCalculator`
- Guarda el `dataset_id` para validaciones posteriores
- Opcionalmente carga datos si `load_full=true`

---

### Paso 2: C√°lculos de M√©tricas (Solo Metadata)
```
GET /actualidad?dataset_id=ijus-ubej
GET /accesibilidad?dataset_id=ijus-ubej
GET /confidencialidad?dataset_id=ijus-ubej
GET /trazabilidad?dataset_id=ijus-ubej
GET /disponibilidad?dataset_id=ijus-ubej
```

**Caracter√≠sticas:**
- ‚úÖ No requieren carga previa de datos
- ‚úÖ Respuesta instant√°nea
- ‚úÖ V√°lidan que el `dataset_id` coincida con inicializaci√≥n

---

### Paso 3: Carga de Datos (Para An√°lisis Profundo)
```
POST /load_data
```

**Qu√© sucede:**
- Descarga hasta 50,000 registros del dataset
- Optimiza tipos de datos
- Prepara DataFrame para an√°lisis
- Permite c√°lculos que requieren datos reales

---

### Paso 4: C√°lculos Avanzados (Con Datos)
```
GET /completitud?dataset_id=ijus-ubej
GET /unicidad?dataset_id=ijus-ubej&nivel_riesgo=1.5
GET /conformidad?dataset_id=ijus-ubej
GET /credibilidad?dataset_id=ijus-ubej
```

**Caracter√≠sticas:**
- ‚ö†Ô∏è Requieren `/load_data` previo
- ‚ö†Ô∏è Devuelven error 400 si datos no est√°n cargados
- ‚úÖ An√°lisis profundo con informaci√≥n real

---

## üìê Detalles T√©cnicos por M√©trica

### 1. üïê ACTUALIDAD

**Definici√≥n:** Mide qu√© tan reciente es la informaci√≥n del dataset respecto a su frecuencia de actualizaci√≥n.

**Escala:** 0-10
- `10.0` = Datos dentro de su frecuencia de actualizaci√≥n
- `0.0` = Datos desactualizados (fuera de frecuencia)
- `5.0` = Informaci√≥n indeterminada

**F√≥rmula:**
```
Si frecuencia = "No aplica":
  actualidad = 5.0

Si frecuencia = "Nunca":
  actualidad = 0.0

Si frecuencia = "M√°s de tres a√±os":
  actualidad = 10.0

Si frecuencia = "Solo una vez" Y hace <= 5 a√±os:
  actualidad = 10.0

Si (fecha_actual - fecha_actualizacion) <= frecuencia_dias:
  actualidad = 10.0

Si (fecha_actual - fecha_actualizacion) > frecuencia_dias:
  actualidad = 0.0
```

**Fuentes de Datos:**
- Metadata: `fecha_actualizacion`, `frecuencia_actualizacion`
- Fallback: Socrata `rowsUpdatedAt`, `frequency`

**Ejemplo:**
```
Dataset actualizado: 2025-11-15
Frecuencia: 30 d√≠as
Fecha actual: 2025-11-28 (13 d√≠as despu√©s)
Resultado: 10.0 ‚úÖ (dentro de frecuencia)

Dataset actualizado: 2025-10-01
Frecuencia: 30 d√≠as
Fecha actual: 2025-11-28 (58 d√≠as despu√©s)
Resultado: 0.0 ‚ùå (fuera de frecuencia)
```

**Endpoint:** `GET /actualidad`

---

### 2. üìç ACCESIBILIDAD

**Definici√≥n:** Eval√∫a qu√© tan f√°cil es acceder y descubrir el dataset mediante metadatos y documentaci√≥n.

**Escala:** 0-10
- `10.0` = Excelente accesibilidad (tags + documentaci√≥n)
- `5.0` = Accesibilidad parcial (solo una fuente)
- `0.0` = Baja accesibilidad (sin metadata)

**F√≥rmula:**
```
puntaje_tags = 5.0 si len(tags) > 0 else 0.0
puntaje_links = 5.0 si existen links (documentaci√≥n/normativa) else 0.0

accesibilidad = puntaje_tags + puntaje_links
accesibilidad = min(max(0, accesibilidad), 10)
```

**Links Evaluados:**
- `attributionLink` (enlace de atribuci√≥n)
- `metadata.custom_fields['Informaci√≥n de Datos']['URL Documentaci√≥n']`
- `metadata.custom_fields['Informaci√≥n de Datos']['URL Normativa']`

**Ejemplo:**
```
Tags: ["salud", "covid-19", "estad√≠sticas"]  ‚Üí puntaje_tags = 5.0
Links encontrados:
  - https://ejemplo.com/docs  ‚Üí puntaje_links = 5.0

accesibilidad = 5.0 + 5.0 = 10.0 ‚úÖ
```

**Endpoint:** `GET /accesibilidad`

---

### 3. üîê CONFIDENCIALIDAD

**Definici√≥n:** Mide el riesgo de exposici√≥n de datos sensibles o personales.

**Escala:** 0-10
- `10.0` = Sin datos sensibles (m√°xima confidencialidad)
- `5.0` = Datos sensibles moderados
- `0.0` = Muchos datos cr√≠ticos expuestos

**F√≥rmula:**
```
Detectar columnas sensibles por palabras clave:

ALTO riesgo (peso=3):
  - documento, c√©dula, pasaporte, contrase√±a, tarjeta, cuenta bancaria
  - historial m√©dico, diagn√≥stico, password, DNI

MEDIO riesgo (peso=2):
  - direcci√≥n, tel√©fono, celular, email, correo

BAJO riesgo (peso=1):
  - nombre, apellido, edad, sexo, fecha nacimiento

propConf = N_columnas_sensibles / N_columnas_totales
riesgo_total = suma(pesos de columnas sensibles)

confidencialidad = max(0, 10 - (propConf √ó riesgo_total))
```

**Ejemplo:**
```
Total columnas: 20
Columnas sensibles detectadas:
  - cedula: peso=3
  - email: peso=2
  - edad: peso=1

N_conf = 3
propConf = 3/20 = 0.15
riesgo_total = 3 + 2 + 1 = 6

confidencialidad = 10 - (0.15 √ó 6) = 10 - 0.9 = 9.1 ‚úÖ
```

**Endpoint:** `GET /confidencialidad`

---

### 4. ‚úÖ COMPLETITUD

**Definici√≥n:** Eval√∫a qu√© porcentaje de celdas del dataset tienen valores (no nulos).

**Escala:** 0-10
- `10.0` = Dataset completamente lleno
- `5.0` = 50% de valores completos
- `0.0` = Muchos valores faltantes

**F√≥rmula:**
```
total_filas = len(df)
total_columnas = len(df.columns)
total_celdas = total_filas √ó total_columnas
total_nulos = df.isna().sum().sum()

proporcion_nulos = total_nulos / total_celdas
proporcion_completo = 1 - proporcion_nulos

completitud = proporcion_completo √ó 10

Penalizaci√≥n por columnas muy incompletas (>50% nulos):
  - Cada columna con >50% nulos: -0.5 puntos
```

**Ejemplo:**
```
Dataset: 1000 filas, 10 columnas
Total celdas: 10,000
Valores nulos: 500 (5%)

proporcion_completo = 1 - 0.05 = 0.95
completitud = 0.95 √ó 10 = 9.5 ‚úÖ

Si 3 columnas tienen >50% nulos:
  completitud = 9.5 - (3 √ó 0.5) = 8.0
```

**Requiere:** `POST /load_data` previo

**Endpoint:** `GET /completitud`

---

### 5. üìã CONFORMIDAD

**Definici√≥n:** Eval√∫a si el dataset cumple con est√°ndares de formato, estructura y convenciones.

**Escala:** 0-1 (o 0-10 en escala normalizada)

**An√°lisis Realizado:**
```
1. Validaci√≥n de Formatos Geogr√°ficos
   - Departamentos de Colombia (validados contra API)
   - Municipios de Colombia
   - C√≥digos DANE

2. Validaci√≥n de Formatos de Fechas
   - Detecta formatos ISO 8601
   - Formatos YYYY-MM-DD
   - An√°lisis de consistencia

3. Validaci√≥n de Tipos de Datos
   - Num√©ricos vs strings
   - Booleanos
   - Fechas

4. Validaci√≥n de Patrones
   - Emails: regex de validaci√≥n
   - URLs: formato v√°lido
   - N√∫meros de tel√©fono
```

**F√≥rmula General:**
```
conformidad = (columnas_validas / columnas_relevantes) √ó 1.0

Si no hay columnas relevantes:
  conformidad = 0.0
```

**Ejemplo:**
```
Dataset con columnas:
  - departamento: "Antioquia" ‚úÖ (v√°lido)
  - municipio: "Bogot√°" ‚ö†Ô∏è (inconsistente con departamento)
  - fecha: "2025-11-30" ‚úÖ (ISO 8601)
  - email: "usuario@example.com" ‚úÖ (v√°lido)

Validaciones pasadas: 3/4
conformidad = 0.75 (en escala 0-1)
```

**Requiere:** Opcionalmente datos (carga autom√°tica si es necesario)

**Endpoint:** `GET /conformidad`

---

### 6. üìç TRAZABILIDAD

**Definici√≥n:** Mide la capacidad de auditar y rastrear la procedencia y cambios en los datos.

**Escala:** 0-10

**F√≥rmula:**
```
trazabilidad = (medida_metadatos_diligenciados √ó 0.75) +
               (medida_acceso_auditado √ó 0.20) +
               (medida_titulo_sin_fecha √ó 0.05)

Donde:

medida_metadatos_diligenciados:
  - Campos esperados: id, name, description, owner, tags, etc. (20 campos)
  - Proporci√≥n completada: campos_diligenciados / campos_totales
  - Penalizaci√≥n cuadr√°tica: (1 - proporci√≥n)¬≤
  - Score: (1 - penalizaci√≥n) √ó 10

medida_acceso_auditado:
  - Verifica: fecha_actualizaci√≥n, propietario, publicador, contacto
  - Ponderaci√≥n con pesos (0.4, 0.3, 0.2, 0.1)
  - Score: suma_pesos √ó 10

medida_titulo_sin_fecha:
  - Si t√≠tulo NO tiene a√±o: 10.0
  - Si t√≠tulo tiene a√±o: 0.0
```

**Ejemplo:**
```
Campos diligenciados: 15/20
Proporci√≥n: 0.75
Penalizaci√≥n: (1-0.75)¬≤ = 0.0625
medida_metadatos = (1-0.0625) √ó 10 = 9.375

Campos cr√≠ticos: 3/5 presentes = 0.6 √ó 10 = 6.0
T√≠tulo sin fecha: "COVID-19 Colombia" ‚úÖ = 10.0

trazabilidad = (9.375 √ó 0.75) + (6.0 √ó 0.20) + (10.0 √ó 0.05)
             = 7.03 + 1.2 + 0.5 = 8.73 ‚úÖ
```

**Endpoint:** `GET /trazabilidad`

---

### 7. üåê DISPONIBILIDAD

**Definici√≥n:** Eval√∫a la capacidad del dataset de estar siempre listo y accesible para su uso.

**Escala:** 0-10

**F√≥rmula:**
```
disponibilidad = (accesibilidad + actualidad) / 2

Escala de interpretaci√≥n:
  - 10: Datos siempre listos y accesibles (m√°ximo)
  - 7-9: Dataset generalmente disponible (bueno)
  - 5-6: Disponibilidad parcial (aceptable)
  - 3-4: Disponibilidad limitada (deficiente)
  - 0-2: Datos pr√°cticamente no disponibles (cr√≠tico)
```

**Ejemplo:**
```
Accesibilidad: 8.0 (buena documentaci√≥n)
Actualidad: 10.0 (reciente)

disponibilidad = (8.0 + 10.0) / 2 = 9.0 ‚úÖ (excelente)
```

**Endpoint:** `GET /disponibilidad`

---

### 8. üì¶ PORTABILIDAD

**Definici√≥n:** Mide si el dataset se puede descargar y usar sin depender de software propietario.

**Escala:** 0-10

**An√°lisis Realizado:**
```
1. Clasificaci√≥n de Formatos
   MUY PORTABLE (peso=10):
     - JSON, CSV, XML, JSONL, GeoJSON
   
   MEDIANAMENTE PORTABLE (peso=6):
     - Excel (XLSX), ODS
   
   NO PORTABLE (peso=2):
     - PDF, DOC, im√°genes
     - Formatos propietarios

2. Criterios de No Portabilidad
   ‚ùå Contiene macros
   ‚ùå Requiere contrase√±a
   ‚ùå Tiene bloqueos de edici√≥n
   ‚ùå Comprimido en ZIP (sin open source)

3. C√°lculo de Score
   portabilidad = (suma_pesos_formatos / (N_formatos √ó 10)) √ó 10
   
   Si tiene restricciones: portabilidad √ó 0.5
```

**Ejemplo:**
```
Formatos disponibles:
  - CSV: peso=10 ‚úÖ
  - JSON: peso=10 ‚úÖ
  - Excel: peso=6 ‚ö†Ô∏è

suma_pesos = 10 + 10 + 6 = 26
portabilidad = (26 / (3 √ó 10)) √ó 10 = 8.67 ‚úÖ
```

**Requiere:** `POST /load_data` previo

**Endpoint:** `GET /portabilidad`

---

### 9. üîó CREDIBILIDAD

**Definici√≥n:** Eval√∫a la confiabilidad del dataset basada en la calidad de metadatos y procedencia.

**Escala:** 0-10

**Componentes:**
```
1. Validez de Metadatos (40%)
   - Campos completos y actualizados
   - Consistencia entre campos
   - Ausencia de valores por defecto gen√©ricos

2. Procedencia (30%)
   - Organizaci√≥n conocida/verificada
   - Fuente documentada
   - Historial de publicaci√≥n

3. Validaciones de Datos (20%)
   - Correlaci√≥n entre campos
   - Outliers razonables
   - Consistencia temporal

4. Informaci√≥n de Contacto (10%)
   - Email de contacto presente
   - Tel√©fono disponible
   - Responsable identificado
```

**F√≥rmula:**
```
credibilidad = (validez_metadatos √ó 0.40) +
               (validez_procedencia √ó 0.30) +
               (validaciones_datos √ó 0.20) +
               (info_contacto √ó 0.10)

credibilidad = credibilidad √ó 10  (escala 0-10)
```

**Requiere:** Opcionalmente datos

**Endpoint:** `GET /credibilidad`

---

### 10. üîÑ RECUPERABILIDAD

**Definici√≥n:** Mide qu√© tan f√°cil es recuperar y reconstruir el dataset y su contexto.

**Escala:** 0-10

**F√≥rmula:**
```
recuperabilidad = (accesibilidad + 
                   metadatos_completos + 
                   metadatos_auditados) / 3

Donde:

metadatos_completos (0-1):
  - Verifica: t√≠tulo, descripci√≥n, etiquetas, schema, contexto
  - Score: campos_presentes / 5

metadatos_auditados (0-1):
  - Verifica: versionado, procedencia, t√©cnicos, contacto, licencia
  - Score: campos_presentes / 5

Normalizaci√≥n final:
  recuperabilidad = recuperabilidad √ó 10
```

**Ejemplo:**
```
Accesibilidad: 8.0
Metadatos completos: 0.8 (4/5 campos)
Metadatos auditados: 0.6 (3/5 campos)

recuperabilidad = (8.0 + (0.8√ó10) + (0.6√ó10)) / 3 = 8.0 ‚úÖ
```

**Requiere:** `POST /load_data` previo

**Endpoint:** `GET /recuperabilidad`

---

### 11. üîë UNICIDAD

**Definici√≥n:** Detecta y cuantifica filas y columnas duplicadas en el dataset.

**Escala:** 0-10
- `10.0` = Sin duplicados (m√°xima unicidad)
- `5.0` = Duplicados moderados
- `0.0` = Muchos duplicados

**Tipos de Duplicados Detectados:**
```
1. FILAS DUPLICADAS EXACTAS
   - Filas con id√©nticos valores en TODAS las columnas
   - Penalizaci√≥n: (N_duplicados / N_total_filas) √ó 10

2. COLUMNAS DUPLICADAS EXACTAS
   - Columnas con id√©nticos valores en TODAS las filas
   - Penalizaci√≥n: (N_columnas_dup / N_total_columnas) √ó 5

3. QUASI-DUPLICADOS (por par√°metro nivel_riesgo)
   - Filas que coinciden en columnas clave
   - Penalizaci√≥n escalada por nivel_riesgo
```

**F√≥rmula:**
```
penalizacion_filas = (num_dup_filas / num_total_filas) √ó 10 √ó nivel_riesgo
penalizacion_columnas = (num_dup_columnas / num_total_columnas) √ó 5

unicidad = max(0, 10 - penalizacion_filas - penalizacion_columnas)
```

**Par√°metros:**
```
nivel_riesgo:
  - 1.0: Penalizaci√≥n suave (datos explorativos)
  - 1.5: Penalizaci√≥n media (RECOMENDADO)
  - 2.0: Penalizaci√≥n estricta (datos cr√≠ticos)
```

**Ejemplo:**
```
Dataset: 1000 filas, 15 columnas
Filas duplicadas exactas: 50
Columnas duplicadas: 2

penalizacion_filas = (50 / 1000) √ó 10 √ó 1.5 = 0.75
penalizacion_columnas = (2 / 15) √ó 5 = 0.67

unicidad = 10 - 0.75 - 0.67 = 8.58 ‚úÖ
```

**Requiere:** `POST /load_data` previo

**Endpoint:** `GET /unicidad?nivel_riesgo=1.5`

---

### 12. ‚≠ê RELEVANCIA

**Definici√≥n:** Eval√∫a si el dataset proporciona valor para la toma de decisiones.

**Escala:** 0-10

**Criterios:**
```
1. Categorizaci√≥n (70%)
   - Dataset est√° bien categorizado
   - Etiquetas son descriptivas
   - Clasificaci√≥n clara

2. Volumen de Datos (30%)
   - M√≠nimo 50 filas para relevancia b√°sica
   - M√°s filas = mayor relevancia
   - F√≥rmula: relevancia_volumen = (filas / 50) √ó 10
```

**F√≥rmula:**
```
medida_categoria = 7.0  (si categorizado)
medida_filas = min(10.0, (num_filas / 50) √ó 10)

relevancia = (medida_categoria + medida_filas) / 2
relevancia = min(10.0, relevancia)
```

**Ejemplo:**
```
Dataset: bien categorizado, 500 filas
medida_categoria = 7.0
medida_filas = (500 / 50) √ó 10 = 100.0 ‚Üí min(10.0) = 10.0

relevancia = (7.0 + 10.0) / 2 = 8.5 ‚úÖ
```

**Endpoint:** `GET /relevancia` (no implementado en endpoints actuales)

---

### 13. ‚öôÔ∏è PRECISI√ìN

**Definici√≥n:** Eval√∫a el nivel apropiado de desagregaci√≥n y detalle en los datos.

**Escala:** 0-10

**An√°lisis:**
```
1. An√°lisis de Cardinalidad
   - Columnas categ√≥ricas: verificar valores √∫nicos
   - Si muchos valores √∫nicos: mayor precisi√≥n
   - Si pocos valores √∫nicos: menor precisi√≥n

2. Granularidad Temporal
   - Datos diarios: m√°xima precisi√≥n
   - Datos mensuales: media precisi√≥n
   - Datos anuales: baja precisi√≥n

3. Informaci√≥n Geogr√°fica
   - Nivel de detalle: pa√≠s, regi√≥n, municipio, vereda
   - Mayor detalle = mayor precisi√≥n
```

**F√≥rmula (General):**
```
precision = (cardinalidad_promedio / cardinalidad_maxima) √ó 10

Donde cardinalidad_maxima se ajusta seg√∫n tipo de dato
```

**Endpoint:** `GET /precision` (no implementado en endpoints actuales)

---

### 14. üîó CONSISTENCIA

**Definici√≥n:** Mide la coherencia y falta de contradicciones en los datos.

**Escala:** 0-10

**Validaciones:**
```
1. Coherencia Referencial
   - Claves for√°neas v√°lidas
   - Referencias cruzadas consistentes

2. Rango de Valores
   - Valores num√©ricos dentro de rangos esperados
   - Fechas l√≥gicamente ordenadas

3. Correlaci√≥n entre Campos
   - Si departamento=X, municipio debe ser de X
   - Relacionales coherentes

4. Tipo de Dato Consistente
   - Columna num√©rica no tiene strings
   - Fechas en formato consistente
```

**F√≥rmula:**
```
N_validaciones_pasadas = cantidad de validaciones sin conflicto
N_validaciones_totales = cantidad total de validaciones

consistencia = (N_pasadas / N_totales) √ó 10
```

**Ejemplo:**
```
Dataset: 1000 filas
Validaciones:
  - Rangos num√©ricos: 1000/1000 pasadas ‚úÖ
  - Fechas ordenadas: 995/1000 pasadas ‚ö†Ô∏è (5 inconsistencias)
  - Referencias: 1000/1000 pasadas ‚úÖ

consistencia = (2995 / 3000) √ó 10 = 9.98 ‚úÖ
```

**Requiere:** `POST /load_data` previo

**Endpoint:** `GET /consistencia` (no implementado en endpoints actuales)

---

## üîß Arquitectura T√©cnica

### Estructura de Clases

```python
class DataQualityCalculator:
    # Inicializaci√≥n
    __init__(dataset_id, metadata)
    
    # Carga de datos
    async load_data(limit=50000)
    _optimize_dtypes()
    
    # M√©tricas (solo metadata)
    calculate_actualidad()
    calculate_accesibilidad_from_metadata()
    calculate_confidencialidad_from_metadata()
    calculate_trazabilidad()
    calculate_disponibilidad()
    
    # M√©tricas (con datos opcionales)
    calculate_conformidad_from_metadata_and_data()
    calculate_credibilidad()
    calculate_recuperabilidad()
    
    # M√©tricas (requieren datos)
    calculate_completitud()
    calculate_unicidad()
    calculate_portabilidad()
    calculate_consistencia()
    calculate_precision()
```

### Endpoints REST

```
POST   /initialize              ‚Üí Inicializa dataset y obtiene metadata
POST   /load_data               ‚Üí Carga datos completos (hasta 50K registros)

GET    /actualidad              ‚Üí Score actualidad (0-10)
GET    /accesibilidad           ‚Üí Score accesibilidad (0-10)
GET    /confidencialidad        ‚Üí Score confidencialidad (0-10) + detalles
GET    /completitud             ‚Üí Score completitud (0-10)
GET    /conformidad             ‚Üí Score conformidad (0-1) + detalles
GET    /trazabilidad            ‚Üí Score trazabilidad (0-10)
GET    /disponibilidad          ‚Üí Score disponibilidad (0-10)
GET    /portabilidad            ‚Üí Score portabilidad (0-10)
GET    /credibilidad            ‚Üí Score credibilidad (0-10)
GET    /recuperabilidad         ‚Üí Score recuperabilidad (0-10)
GET    /unicidad                ‚Üí Score unicidad (0-10)
```

---

## üìä Ejemplo de Flujo Completo

```bash
# 1. Inicializar
curl -X POST http://localhost:8001/initialize \
  -H "Content-Type: application/json" \
  -d '{
    "dataset_id": "ijus-ubej",
    "load_full": false
  }'

Respuesta:
{
  "message": "Dataset initialized successfully",
  "dataset_id": "ijus-ubej",
  "dataset_name": "Procesos Judiciales",
  "rows": 0,
  "columns": 0,
  "metadata_obtained": true
}

# 2. Calcular m√©tricas (solo metadata)
curl http://localhost:8001/actualidad?dataset_id=ijus-ubej
‚Üí {"score": 10.0}

curl http://localhost:8001/accesibilidad?dataset_id=ijus-ubej
‚Üí {"score": 8.5, "details": {...}}

# 3. Cargar datos (para an√°lisis profundos)
curl -X POST http://localhost:8001/load_data
Respuesta:
{
  "message": "Full data loaded successfully",
  "rows": 50000,
  "columns": 45
}

# 4. Calcular m√©tricas avanzadas
curl http://localhost:8001/unicidad?dataset_id=ijus-ubej&nivel_riesgo=1.5
‚Üí {"score": 9.2}

curl http://localhost:8001/completitud?dataset_id=ijus-ubej
‚Üí {"score": 8.8}
```

---

## üîç Optimizaciones Implementadas

1. **Paginaci√≥n Inteligente**: 
   - Descarga m√°ximo 50K registros
   - Detecta √∫ltima p√°gina autom√°ticamente
   - Evita timeout en datasets grandes

2. **Cach√© de Conversiones**:
   - Frecuencias convertidas a d√≠as (c√°lculos posteriores r√°pidos)
   - Departamentos de Colombia (API local)

3. **Optimizaci√≥n de Tipos**:
   - Convierte strings largos a categor√≠as
   - Reduce memoria consumida

4. **Validaci√≥n de Consistencia**:
   - Cada endpoint valida `dataset_id`
   - Previene mezcla de datasets
   - Errores claros si hay mismatch

---

## üìà Matriz de Requisitos

| M√©trica | Metadata | Datos | Async | Cache |
|---------|----------|-------|-------|-------|
| Actualidad | ‚úÖ | ‚ùå | ‚ùå | ‚úÖ |
| Accesibilidad | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |
| Confidencialidad | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |
| Completitud | ‚úÖ | ‚úÖ* | ‚ùå | ‚ùå |
| Conformidad | ‚úÖ | ‚úÖ* | ‚ùå | ‚úÖ |
| Trazabilidad | ‚úÖ | ‚ùå | ‚ùå | ‚úÖ |
| Disponibilidad | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |
| Portabilidad | ‚úÖ | ‚úÖ | ‚ùå | ‚ùå |
| Credibilidad | ‚úÖ | ‚úÖ* | ‚ùå | ‚ùå |
| Recuperabilidad | ‚úÖ | ‚úÖ | ‚ùå | ‚ùå |
| Unicidad | ‚úÖ | ‚úÖ | ‚ùå | ‚ùå |

*Nota: ‚úÖ* significa que es opcional pero mejorado si est√° disponible

---

## üöÄ Uso Recomendado

### Para Evaluaci√≥n R√°pida:
```
1. POST /initialize (load_full=false)
2. GET /actualidad
3. GET /accesibilidad
4. GET /confidencialidad
5. GET /trazabilidad
6. GET /disponibilidad
```
**Tiempo:** < 2 segundos

### Para Evaluaci√≥n Completa:
```
1. POST /initialize (load_full=false)
2. POST /load_data
3. GET todas las m√©tricas
```
**Tiempo:** 5-15 segundos (seg√∫n tama√±o dataset)

---

## üìù Validaciones de Entrada

Todos los endpoints requieren:
```
dataset_id (string):
  - No nulo
  - Coincide con dataset inicializado
  - Formato v√°lido (alphanum√©ricas + guiones)
```

---
