# GuÃ­a TÃ©cnica - Data Quality Assessment Backend

## ğŸ“ Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Cliente (Frontend/CLI)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP REST
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FastAPI Server                           â”‚
â”‚  - main.py (17 endpoints)                                    â”‚
â”‚  - CORS Middleware                                           â”‚
â”‚  - Request Validation (Pydantic)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                â–¼                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Metadataâ”‚     â”‚ Data Loading â”‚  â”‚ Calculation  â”‚
   â”‚ Cache   â”‚     â”‚ (Sodapy)     â”‚  â”‚ Engine       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                â–¼                â–¼
        â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  DataQualityCalculator     â”‚
                   â”‚  (data_quality_calculator) â”‚
                   â”‚  - 17 mÃ©tricas             â”‚
                   â”‚  - Validadores            â”‚
                   â”‚  - Parsers                â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼           â–¼            â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Socrata  â”‚  â”‚Spacy   â”‚  â”‚Sklearn â”‚
              â”‚API       â”‚  â”‚(NLP)   â”‚  â”‚(ML)    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  datos.gov.co (Socrata) â”‚
         â”‚  - Metadata API         â”‚
         â”‚  - Resource API (datos) â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Flujos Principales

### Flujo 1: InicializaciÃ³n de Dataset

```python
POST /initialize
â”œâ”€ dataset_id: str (ej: "ijus-ubej")
â””â”€ load_full: bool (default: False)
    â”‚
    â”œâ”€ obtener_metadatos_socrata(dataset_id)
    â”‚  â””â”€ GET {SOCRATA_BASE_URL}/api/views/{dataset_id}
    â”‚     â””â”€ Retorna: nombre, descripciÃ³n, tags, columnas, etc.
    â”‚
    â”œâ”€ DataQualityCalculator.__init__(dataset_id, metadata)
    â”‚  â””â”€ Inicializa: dataset_id, metadata, df=None
    â”‚     â””â”€ Carga listas estÃ¡ticas: departamentos, municipios (en memoria)
    â”‚
    â””â”€ Retorna: DatasetInfoResponse
       â”œâ”€ dataset_name
       â”œâ”€ rows (0 si no load_full)
       â”œâ”€ columns
       â”œâ”€ data_url
       â”œâ”€ metadata_obtained: bool
       â””â”€ total_records_available
```

### Flujo 2: Carga de Datos (Opcional)

```python
POST /load_data
â”‚
â”œâ”€ calculator.load_data(limit=50000)
â”‚  â”œâ”€ Sodapy client (autenticado)
â”‚  â”‚  â””â”€ client.get(dataset_id, limit=50000)
â”‚  â”‚
â”‚  â”œâ”€ PaginaciÃ³n automÃ¡tica (Sodapy maneja internamente)
â”‚  â”‚  â””â”€ Ej: 50000 registros = ~500 requests de 100 cada uno
â”‚  â”‚
â”‚  â”œâ”€ df = pd.DataFrame.from_records(results)
â”‚  â”‚
â”‚  â””â”€ _optimize_dtypes()
â”‚     â”œâ”€ int64 â†’ int8/int16/int32 (si es posible)
â”‚     â”œâ”€ float64 â†’ float32
â”‚     â””â”€ object â†’ category (si hay <10% Ãºnicos)
â”‚
â””â”€ Retorna: DataFrame optimizado en memoria
   â””â”€ self.df (ahora disponible para cÃ¡lculos)
```

### Flujo 3: CÃ¡lculo de MÃ©trica (Ejemplo: Actualidad)

```python
GET /actualidad?dataset_id="ijus-ubej"
â”‚
â”œâ”€ ValidaciÃ³n
â”‚  â””â”€ if calculator.dataset_id != dataset_id â†’ Error 400
â”‚
â”œâ”€ calculator.calculate_actualidad(metadata)
â”‚  â”‚
â”‚  â”œâ”€ Obtener "fecha_actualizacion" del metadata
â”‚  â”‚  â””â”€ Parsear a datetime
â”‚  â”‚
â”‚  â”œâ”€ Calcular dÃ­as desde actualizaciÃ³n
â”‚  â”‚  â””â”€ (now - fecha_actualizacion).days
â”‚  â”‚
â”‚  â”œâ”€ Obtener "frecuencia_actualizacion_dias" (si existe)
â”‚  â”‚  â””â”€ Si no existe: inferir de campo "periodicidad" (ej: "Mensual" â†’ 30 dÃ­as)
â”‚  â”‚
â”‚  â”œâ”€ Aplicar fÃ³rmula:
â”‚  â”‚  â””â”€ Si dÃ­as_desde_actualizaciÃ³n â‰¤ frecuencia_esperada:
â”‚  â”‚     â”œâ”€ score = 10
â”‚  â”‚     â””â”€ Else:
â”‚  â”‚        â”œâ”€ penalizaciÃ³n = 0.5 * (dÃ­as_exceso / frecuencia_esperada)
â”‚  â”‚        â””â”€ score = max(0, 10 - penalizaciÃ³n)
â”‚  â”‚
â”‚  â””â”€ Retornar detalles: {"days_since_update": N, "frequency_days": M, ...}
â”‚
â””â”€ Retorna: ScoreResponse
   â”œâ”€ score: 8.5 (0-10)
   â””â”€ details: {...}
```

### Flujo 4: ValidaciÃ³n de Conformidad (Caso Complejo)

```python
GET /conformidad?dataset_id="ijus-ubej"
â”‚
â”œâ”€ Si NO hay datos cargados:
â”‚  â”œâ”€ Intentar cargar muestra (5000 registros)
â”‚  â””â”€ Si falla: score = 10.0 (sin columnas para validar)
â”‚
â”œâ”€ _detect_relevant_columns(metadata)
â”‚  â”œâ”€ Buscar en columnas por patrones:
â”‚  â”‚  â”œâ”€ "depart*" â†’ departamentos
â”‚  â”‚  â”œâ”€ "munic*" â†’ municipios
â”‚  â”‚  â”œâ”€ "aÃ±o|year|fecha" â†’ aÃ±os
â”‚  â”‚  â”œâ”€ "lat*|latitud" â†’ latitudes
â”‚  â”‚  â”œâ”€ "lon*|longitud" â†’ longitudes
â”‚  â”‚  â””â”€ "correo|email" â†’ emails
â”‚  â”‚
â”‚  â””â”€ Retorna: {"departamentos": [...], "municipios": [...], ...}
â”‚
â”œâ”€ Si NO hay columnas relevantes:
â”‚  â””â”€ score = 10.0 (nada que validar)
â”‚
â”œâ”€ Si SÃ hay columnas relevantes:
â”‚  â”œâ”€ Para cada fila:
â”‚  â”‚  â”œâ”€ Validar departamentos contra lista de 32 oficiales
â”‚  â”‚  â”œâ”€ Validar municipios contra lista de 1,122 oficiales
â”‚  â”‚  â”œâ”€ Validar aÃ±os: 1900 â‰¤ aÃ±o â‰¤ 2100
â”‚  â”‚  â”œâ”€ Validar coordenadas: -90 â‰¤ lat â‰¤ 90, -180 â‰¤ lon â‰¤ 180
â”‚  â”‚  â””â”€ Validar emails: regex RFC 5322
â”‚  â”‚
â”‚  â”œâ”€ Contar valores vÃ¡lidos vs totales
â”‚  â”‚  â””â”€ proporciÃ³n_valida = vÃ¡lidos / total
â”‚  â”‚
â”‚  â”œâ”€ Aplicar fÃ³rmula:
â”‚  â”‚  â””â”€ score = proporciÃ³n_valida * 10
â”‚  â”‚     â””â”€ Rango: 0 (ninguno vÃ¡lido) - 10 (todos vÃ¡lidos)
â”‚  â”‚
â”‚  â””â”€ Retornar detalles: {"valid_count": 450, "total_count": 500, ...}
â”‚
â””â”€ Retorna: ScoreResponse
   â”œâ”€ score: 9.0
   â””â”€ details: {...}
```

## ğŸ§® FÃ³rmulas de MÃ©tricas Clave

### Actualidad (Timeliness)
```
dÃ­as_desde_actualizaciÃ³n = hoy - fecha_Ãºltima_actualizaciÃ³n
frecuencia_esperada = metadata.frecuencia_actualizacion_dias

Si dÃ­as_desde_actualizaciÃ³n â‰¤ frecuencia_esperada:
    score = 10
Else:
    penalizaciÃ³n = 0.5 * (dÃ­as_exceso / frecuencia_esperada)
    score = max(0, 10 - penalizaciÃ³n)
```

### Completitud (Completeness)
```
nulos_por_columna = count(NULL) en cada columna
proporciÃ³n_nulos = nulos_por_columna / total_filas

Para cada columna:
    score_columna = (1 - proporciÃ³n_nulos) * 10

score_final = promedio(score_columna para todas las columnas)
```

### Disponibilidad (Availability)
```
disponibilidad = (accesibilidad + actualidad) / 2
```

### Recuperabilidad (Recoverability)
```
recuperabilidad = (accesibilidad + metadatos_completos + metadatos_auditados) / 3
```

### Unicidad (Uniqueness)
```
filas_duplicadas = count(filas exactamente iguales)
columnas_duplicadas = count(columnas exactamente iguales)

proporciÃ³n_duplicadas = (filas_duplicadas + columnas_duplicadas) / total

penalizaciÃ³n = nivel_riesgo * proporciÃ³n_duplicadas
score = max(0, 10 - penalizaciÃ³n)
```

### Credibilidad (Credibility)
```
credibilidad = (metadata_completeness * 0.4 +
                exactitud_data * 0.3 +
                consistencia_data * 0.3)
```

## ğŸ“Š Validadores Especializados

### 1. Validador de Departamentos Colombianos
```python
DEPARTAMENTOS_VALIDOS = [
    'Amazonas', 'Antioquia', 'Arauca', 'AtlÃ¡ntico', 'BogotÃ¡ D.C.', 
    'BolÃ­var', 'BoyacÃ¡', 'Caldas', 'CaquetÃ¡', 'Casanare', 'Cauca', 
    'Cesar', 'ChocÃ³', 'CÃ³rdoba', 'Cundinamarca', 'GuainÃ­a',
    'Guaviare', 'Huila', 'La Guajira', 'Magdalena', 'Meta', 'NariÃ±o', 
    'Norte de Santander', 'Putumayo', 'QuindÃ­o', 'Risaralda', 
    'San AndrÃ©s y Providencia', 'Santander', 'Sucre', 'Tolima', 
    'Valle del Cauca', 'VaupÃ©s', 'Vichada'
]
# Total: 32 departamentos

ValidaciÃ³n case-insensitive con normalizaciÃ³n de tildes
```

### 2. Validador de Municipios
```python
# 1,122 municipios colombianos
# Estructura: {nombre_municipio: departamento}

ValidaciÃ³n case-insensitive con normalizaciÃ³n de tildes
```

### 3. Validador de Coordenadas GeogrÃ¡ficas
```python
def _is_valid_latitude(value) -> bool:
    try:
        lat = float(value)
        return -90 <= lat <= 90
    except:
        return False

def _is_valid_longitude(value) -> bool:
    try:
        lon = float(value)
        return -180 <= lon <= 180
    except:
        return False
```

### 4. Validador de Emails
```python
# Regex RFC 5322 simplificado:
# ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$

ValidaciÃ³n con manejo de excepciones
```

## ğŸ” Manejo de Errores

### Error 400: Bad Request
```json
{
  "detail": "Dataset not initialized. Call /initialize first."
}
```
**Causas**: 
- Llamar mÃ©trica sin inicializar
- Dataset_id no coincide

### Error 400: Dataset Mismatch
```json
{
  "detail": "Dataset mismatch. Initialized: ijus-ubej, Requested: xyz-abc"
}
```
**SoluciÃ³n**: Reinicializar con nuevo dataset_id

### Error 500: Data Loading Failed
```json
{
  "detail": "Error loading data: [details de la excepciÃ³n]"
}
```
**Causas**:
- Timeout en Socrata
- Credenciales invÃ¡lidas
- Falta de conectividad

## ğŸš€ Optimizaciones Implementadas

### 1. Lazy Loading de Datos
- **Problema**: Datasets de 100K registros saturaban memoria
- **SoluciÃ³n**: No cargar datos por defecto; solo al llamar `/load_data`
- **Beneficio**: Endpoints de metadata son <100ms

### 2. CachÃ© de Metadatos
- **ImplementaciÃ³n**: `calculator.cached_scores = {}`
- **Uso**: Evitar recalcular mismos scores
- **LimitaciÃ³n**: Se limpia al reinicializar

### 3. OptimizaciÃ³n de Tipos de Datos
```python
# Antes: int64 usa 8 bytes por valor
# DespuÃ©s: int8 usa 1 byte (si valores 0-127)
# Ahorro: 8x para datos pequeÃ±os, ~50% promedio

AutomatizaciÃ³n en _optimize_dtypes():
- int64 â†’ int8/int16/int32
- float64 â†’ float32
- object â†’ category
```

### 4. PaginaciÃ³n AutomÃ¡tica (Sodapy)
- **Internamente**: Sodapy pagina automÃ¡ticamente (100 rec/req)
- **Transparencia**: Usuario ve un Ãºnico DataFrame
- **Control**: Configurable via `DEFAULT_RECORDS_LIMIT`

## ğŸ”Œ IntegraciÃ³n de Dependencias Externas

### Socrata (Sodapy)
```python
from sodapy import Socrata

client = Socrata(
    domain="www.datos.gov.co",
    app_token="API_KEY",
    username="user",
    password="pass"
)

# Obtener metadatos
metadata = requests.get(f"https://www.datos.gov.co/api/views/{dataset_id}").json()

# Obtener datos
data = client.get(dataset_id, limit=50000)
```

### Pandas
```python
# ConversiÃ³n de registros a DataFrame
df = pd.DataFrame.from_records(data)

# OptimizaciÃ³n automÃ¡tica
df = df.astype({'column': 'category'})
```

### Scikit-learn (TfidfVectorizer)
```python
# Para cÃ¡lculo de similitud de texto en conformidad
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform([text1, text2])
similarity = cosine_similarity(tfidf_matrix)[0][1]
```

### Spacy
```python
# Para NLP avanzado (opcional)
import spacy
nlp = spacy.load("es_core_news_sm")
doc = nlp("Texto a procesar")
```

## ğŸ“ˆ Monitoreo y Debugging

### Logs AutomÃ¡ticos
```
ğŸš€ Inicializando dataset con ID: ijus-ubej
ğŸ” Obteniendo metadatos desde: https://www.datos.gov.co/api/views/ijus-ubej
âœ… Metadatos obtenidos exitosamente
ğŸ”— Obteniendo datos (sodapy) para dataset: ijus-ubej
ğŸ¯ Registros obtenidos (sodapy): 5000
ğŸ“Š DataFrame creado: 5000 filas, 15 columnas
```

### Indicadores de Estado
- `metadata_obtained: bool` - Metadatos disponibles
- `limit_reached: bool` - Datos truncados (50K mÃ¡ximo)
- `records_count: int` - Registros cargados
- `total_records_available: int` - Disponibles en Socrata

---

**VersiÃ³n**: 1.0  
**Ãšltima actualizaciÃ³n**: 30 de noviembre de 2025
